Exhaustive API Reference for the Cerebras Cloud Node.js SDK (@cerebras/cerebras_cloud_sdk)I. Foundation and InitializationThe Cerebras Cloud Software Development Kit (SDK) for Node.js provides a robust, idiomatic interface for interacting with the Cerebras REST API, enabling developers to integrate high-performance large language model (LLM) inference capabilities into server-side JavaScript and TypeScript environments. This documentation serves as a comprehensive reference for all methods, required parameters, and advanced configuration options available within the @cerebras/cerebras_cloud_sdk package.1.1. Technical Prerequisites and Supported RuntimesSuccessful implementation of the SDK requires specific technical environments to ensure stability and exploit modern asynchronous features. The library is built to support contemporary JavaScript runtimes, including:Requirements Check: The SDK necessitates Node.js version 18 LTS or later (non-End-of-Life versions).1Installation: Installation is handled through the standard Node Package Manager (npm) utility 2:Bashnpm install @cerebras/cerebras_cloud_sdk
Runtime Compatibility: Beyond standard Node.js environments, the SDK is explicitly designed for compatibility with several modern serverless and edge computing platforms, enhancing its utility across diverse deployment architectures. Supported runtimes include Deno v1.28.0 or higher, Bun 1.0 or later, Cloudflare Workers, and the Vercel Edge Runtime.1 It should be noted that environments like React Native are not currently supported.11.2. Client Instantiation (new Cerebras(...)) and Operational ConfigurationThe first step in utilizing the SDK is instantiating the main client object, which manages authentication and foundational network settings. The client is initialized by passing a configuration object to the Cerebras constructor.API Key ManagementAuthentication is managed primarily through the apiKey property. While the API key can be passed explicitly during instantiation, standard best practice involves setting the key as an environment variable (CEREBRAS_API_KEY).2 If the key is present in the environment, it can typically be omitted from the constructor arguments, as the SDK handles auto-discovery.TypeScriptimport Cerebras from '@cerebras/cerebras_cloud_sdk';

// Uses the CEREBRAS_API_KEY environment variable by default
const client = new Cerebras({}); 
Constructor Options Deep Dive: warmTCPConnectionA critical component of the client initialization is the handling of network connection warming, which significantly impacts the performance profile of the application.Default Behavior and TTFT Optimization: By default, the warmTCPConnection parameter is set to true. This instructs the SDK, upon construction, to send a few initial requests to the /v1/tcp_warming endpoint in the background.2 This pre-emptive action is intended to reduce the Time to First Token (TTFT) for subsequent inference requests by establishing and priming network sockets, which is highly beneficial for persistent server environments where the client instance is constructed once and reused.2Performance Implication in Ephemeral Environments: Developers must understand the causal relationship between client construction and performance overhead in dynamic environments. If the client instance is repeatedly reconstructed (e.g., within a serverless function that cold-starts for every request), the TCP warming mechanism will execute repeatedly. This leads to poor performance, as the overhead of establishing and warming connections counteracts the intended TTFT gain.2Recommendation: For applications deployed in serverless, edge, or any ephemeral runtime environments where client instances are frequently created and destroyed, it is strongly recommended to set warmTCPConnection: false in the constructor to eliminate this unnecessary initialization overhead. In contrast, in persistent servers (e.g., long-running containers), a single, global client instantiation should be favored, leveraging the default true setting for optimal TTFT performance.II. The Core Interface: Chat Completions (client.chat.completions.create)The chat.completions endpoint represents the primary interface for conversational AI tasks, adhering to modern LLM API standards that use a structured array of messages.2.1. Method Signature and Type ContractThe create method is asynchronous, returning a promise or an asynchronous iterable, depending on the request parameters. The SDK provides strong TypeScript definitions (Cerebras.Chat.ChatCompletionCreateParams and Cerebras.Chat.ChatCompletion) that define the contract.2Full SignatureThe full TypeScript signature reflects the method’s dual nature for non-streaming and streaming calls:TypeScriptclient.chat.completions.create(
    params: Cerebras.Chat.ChatCompletionCreateParams, 
    options?: RequestOptions
): Promise<Cerebras.Chat.ChatCompletion> | AsyncIterable<Cerebras.Chat.ChatCompletionChunk>
The return type shifts based on the presence of the stream: true parameter within the request body.22.2. Standard Asynchronous Usage (Non-Streaming)For standard, synchronous retrieval of a complete response, the method is awaited, yielding a full Cerebras.Chat.ChatCompletion object.2Response Structure (Cerebras.Chat.ChatCompletion)The non-streaming response object is rich with metadata, including standard fields like id, created, model, and detailed usage statistics (prompt_tokens, completion_tokens, total_tokens).3A particularly valuable element for performance analysis and monitoring is the time_info object, which provides granular latency metrics. This data is essential for diagnosing bottlenecks in production environments, as it segments the total processing time into distinct phases:queue_time: Time spent waiting for execution resources.prompt_time: Time taken for processing the input prompt (tokenization and first pass).completion_time: Time taken for generating the response tokens.total_time: The cumulative time for the entire request lifecycle.32.3. Exhaustive Parameter Reference for ChatCompletionCreateParamsThe params object, which constitutes the first argument to create, defines the LLM request. The following parameters are crucial for defining the conversation and generation constraints:Table Title: Chat Completions Parameter Reference (Core)Parameter NameType (TypeScript)Required/OptionalDescriptionmessagesArray<ChatCompletionMessage>RequiredA sequence of message objects, each containing a role (e.g., user, assistant, system) and content. System prompts must be passed as part of this array using the system role.3modelstringRequiredThe identifier for the large language model to execute the request (e.g., llama3.1-8b, gpt-oss-120b, zai-glm-4.6).3streambooleanOptionalIf set to true, the API streams response chunks via Server Sent Events (SSE). Default is false.2max_completion_tokensnumber | nullOptionalLimits the maximum number of tokens the model generates in its response. This includes any generated reasoning tokens.3temperaturenumberOptionalControls the randomness of the output (sampling temperature). Typical range is 0.0 to 2.0. Default is 1.4top_pnumberOptionalThreshold for nucleus sampling. Tokens are sampled from the smallest set whose cumulative probability exceeds top_p. Default is 1.42.4. Model-Specific Parameters (Reasoning Control)A notable architectural aspect of the Cerebras API is the use of model-specific parameters to control advanced features like reasoning. This configuration requires developers to dynamically tailor the request payload based on the selected model identifier, rather than relying on a single, unified parameter set.4Reasoning allows certain models to generate transparent, step-by-step logic—or reasoning tokens—prior to producing the final output, offering insights into the model's thought process.4Table Title: Conditional Reasoning ParametersParameter NameType (TypeScript)Target Model(s)Description & Levelsreasoning_effort'low' | 'medium' | 'high'gpt-oss-120bControls the depth and length of the reasoning trace. Setting to 'low' provides minimal reasoning and faster responses, while 'high' provides extensive analysis. 'medium' is the default.4disable_reasoningbooleanzai-glm-4.6Used to explicitly override the default behavior of this model family, toggling reasoning generation off.4The presence of these distinct parameters underscores the importance of maintaining awareness of the specific capabilities and required input formats for each deployed model.III. Streaming API (stream: true) ImplementationThe Cerebras SDK provides explicit support for streaming responses using Server Sent Events (SSE), a critical feature for applications requiring low latency or real-time display of generation progress.3.1. Detailed SSE Handling in Node.jsWhen the stream parameter is set to true, the client.chat.completions.create method returns an asynchronous iterable object, rather than a single promise resolving to a complete response.1 This structure is optimized for modern Node.js constructs, specifically the for await...of loop, which processes chunks as they arrive from the API.1TypeScriptimport Cerebras from '@cerebras/cerebras_cloud_sdk';
//... client initialization...

async function streamingExample() {
    const stream = await client.chat.completions.create({
        messages:,
        model: 'llama3.1-8b',
        stream: true,
    });

    for await (const chunk of stream) {
        process.stdout.write(chunk.choices?.delta?.content |

| '');
    }
}
streamingExample();
3.2. Response Chunk Analysis (ChatCompletionChunk)The structure of the data received during streaming (ChatCompletionChunk) differs fundamentally from the non-streaming response. Each chunk contains partial, incremental data, and the content must be concatenated by the client application to reconstruct the final response.Extraction Path: For Chat Completions, the generated text is extracted through the nested path: chunk.choices?.delta?.content.1 This optional chaining handles empty delta objects often sent as keep-alives or introductory stream information.End-of-Stream Handling: The stream is naturally closed by the SDK once the API signals completion. The final chunk typically contains the finish_reason (e.g., "stop," "length," or "content_filter"), which provides metadata on why generation halted, confirming the successful conclusion of the asynchronous iteration.IV. Advanced Functionality: Structured Outputs and Reasoning OutputThe Cerebras API supports advanced capabilities that move beyond simple text generation, allowing for robust integration into complex data processing pipelines and applications requiring transparency.4.1. Structured Outputs using JSON Schema EnforcementFor use cases like function calling, data extraction, and reliable machine-to-machine communication, structured outputs are essential. The SDK facilitates strict schema enforcement, ensuring the model's output conforms precisely to a defined JSON structure.This capability is controlled via the optional response_format parameter, which requires a specific nested object structure 3:The required type field must be set to "json_schema".The json_schema object must contain the actual schema definitions.The schema itself must follow the standard JSON Schema format and specify several required properties for Cerebras enforcement 3:TypeScript// Example definition of the response_format parameter
const structuredParams: Cerebras.Chat.ChatCompletionCreateParams = {
    //... other parameters...
    response_format: {
        "type": "json_schema",
        "json_schema": {
            "name": "schema_name", // Unique identifier for the schema
            "strict": true,       // Enforces strict adherence
            "schema": {           // The actual JSON Schema definition object
                //... schema properties...
            }
        }
    }
};
This enforcement mechanism gives developers the power to define the exact output contracts, eliminating the need for brittle post-processing or retry loops that characterize systems relying solely on natural language responses.4.2. Accessing Reasoning Trace DataWhen models that support reasoning (such as gpt-oss-120b or zai-glm-4.6) are configured to generate their thought process via model-specific parameters (Section 2.4), this explanatory trace is included directly in the non-streaming response object.4The generated reasoning trace, which outlines the model's step-by-step logic used to arrive at the final answer, is accessible as a string field within the message structure 3:Path: chatCompletion.choices.message.reasoningThe inclusion of this field is a crucial architectural difference that enhances transparency. It allows applications to use the LLM not just for the output, but also for auditing, verification, or debugging purposes, which is critical in regulated or high-stakes generative processes.V. Legacy Interface: Text Completions (client.completions.create)While chat.completions is the modern, preferred interface for conversational AI, the SDK also provides access to the traditional, legacy Text Completions interface via client.completions.create. This endpoint is simpler, typically used for single-turn generative tasks that do not require explicit message role definitions.5.1. Method Signature and Structural DifferencesThe primary structural distinction lies in the input format. Instead of the complex messages array used by the chat interface, Text Completions accept a single prompt string.1The method signature mirrors the chat interface in terms of its asynchronous nature and handling of streaming requests.5.2. Text Completions Parameter Reference and OutputThe request body for client.completions.create requires the prompt string and the model identifier. Other generative constraints, such as max_tokens and stream, are available and function similarly to their counterparts in the chat interface.1When streaming is enabled via this interface, the method returns an AsyncIterable. However, the path used to extract the content chunk differs from the chat endpoint. Text completion chunks contain the content in a dedicated text field:Output Retrieval (Streaming): chunk.choices?.text.1This difference necessitates conditional logic in applications that intend to consume both Chat Completion streams and Text Completion streams using a unified processor.VI. Advanced Client Configuration and Request OverridesProduction-grade deployment often requires fine-tuning network behavior, timeout management, and error handling beyond the default SDK settings. The Cerebras SDK provides a flexible mechanism to override client configuration settings on a per-request basis.6.1. Request-Specific Overrides (The Second Argument)Any create method call accepts an optional second argument, often referenced as RequestOptions. This object allows developers to temporarily customize HTTP behavior for that specific API interaction without modifying the global client configuration.2These overrides are paramount for operational robustness, allowing engineers to set precise latency controls and network parameters for critical or asynchronous operations.Table Title: Per-Request Configuration Options (RequestOptions)Option NameType (TypeScript)Purpose and Rationaletimeoutnumber (milliseconds)Specifies the maximum duration the request is allowed to take before being cancelled. This overrides the client's default timeout setting, allowing strict latency limits on individual API calls.2maxRetriesnumber | 0Configures the automatic retry logic implemented by the SDK for handling transient network or server errors. Setting this to 0 disables retries, which is advisable for non-idempotent or latency-sensitive operations.2httpAgenthttp.Agent | https.AgentAllows passing a custom Node.js HTTP Agent instance. This is essential for advanced networking requirements, such as integrating with proxies, managing custom socket pooling, or leveraging specific connection settings.2signalAbortSignalAllows the request to be cancelled dynamically by associating it with a standard AbortController. This is crucial for user-facing applications where a generation may need to be stopped mid-stream.6.2. Advanced Debugging and Error ManagementThe SDK facilitates advanced debugging and integration with custom logging pipelines by exposing methods to retrieve raw HTTP data.Accessing Raw HTTP Data: The promise-like object returned by the non-streaming create method (APIPromise) includes utility functions for accessing the underlying network response object 2:.asResponse(): Returns the raw fetch() Response data object, allowing inspection of HTTP headers, status codes, and other low-level network details..withResponse(): Returns a tuple containing both the parsed JSON response data and the raw Response object, combining ease of use with full debugging capabilities.Handling Undocumented ParametersThe SDK adheres to strong typing via TypeScript definitions. However, the library is designed not to enforce runtime validation, offering flexibility for developers to use new or experimental API parameters before they are officially integrated into the SDK's definitions. If a developer needs to send a parameter not yet recognized by the SDK's TypeScript types, they can bypass type checking using the standard TypeScript escape hatch: // @ts-expect-error.1 This approach sends the extra key/value pairs as-is in the request payload, enabling immediate access to bleeding-edge features without waiting for an SDK update.VII. ConclusionsThe @cerebras/cerebras_cloud_sdk for Node.js provides a deeply integrated, highly configurable interface for Cerebras inference services. The SDK is distinguished by its separation of Chat and Text completion interfaces, its comprehensive support for Server Sent Events streaming via asynchronous iterables, and its integration of specialized LLM features like reasoning control and structured JSON output enforcement.For developers building high-performance applications, meticulous attention must be paid to operational configurations, particularly the impact of the default TCP warming mechanism on serverless function performance. By utilizing the available request-specific overrides (timeout, maxRetries, httpAgent), developers can ensure the robust and precise control necessary for demanding production environments. The SDK's strong TypeScript definitions, combined with the ability to access raw HTTP responses for debugging and bypass typing for early feature adoption, solidify its status as an expert-level tool for large language model integration.